{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Clase - Computación Distribuida\n",
    "\n",
    "    ## Pyspark Hands-on \n",
    "    #### Marcelo Medel Vergara - Diplomado Data Engineer USACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizas Anaconda para administrar ambientes de desarrollo la mejor vía para asegurar que funcionen correctamente las librerías es instalando directo desde Conda. Adicionalmente se instalan todas las librerías que necesita Spark.\n",
    "- `conda create -n pyspark-DE`\n",
    "- `conda activate pyspark-DE`\n",
    "- `conda install -c conda-forge pyspark python=3.10`\n",
    "\n",
    "De igual forma se puede instalar a través de PIP, pero sin asegurar funcionamiento correcto ni la instalación de dependencias. Adicionalmente será necesario tener instalado Pandas.\n",
    "- `conda create -n pyspark-DE python=3.10`\n",
    "- `conda activate pyspark-DE`\n",
    "- `pip install pyspark`\n",
    "- `pip install pandas`.\n",
    "\n",
    "Para Google Colab:\n",
    "- `!pip install pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession es una clase en PySpark que existe desde la versión 2.0 (2016) que simplifica la forma de trabajar con Spark, tanto en las configuraciones como en la manipulación de datos estructurados. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcionalidades principales de SparkSession:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Configura Spark**: Para profundizar en las configuraciones posibles de Spark, visite https://spark.apache.org/docs/latest/configuration.html.\n",
    "    - `SparkSession.builder.appName(\"some name\").**config(\"some config key,value\")**.getOrCreate()`\n",
    "    - `spark.conf.get(\"some config key\")`\n",
    "\n",
    "2. **Crear DataFrames**: permite leer y escribir (Input/Output) diversas fuentes de datos y crear DataFrames para la manipulación de datos.\n",
    "    - `spark.createDataFrame(data [, schema])`\n",
    "    - `spark.read.json(\"path to some json\")`\n",
    "\n",
    "3. **Ejecutar SQL**: facilita la ejecución de consultas en SQL sobre los DataFrames.\n",
    "    - `spark.sql(\"query to some view\")`\n",
    "    \n",
    "4. **Gestiona contexto de Spark**: facilita la configuración y acceso a diferentes componentes y funcionalidades de Spark\n",
    "    - `spark.sql.shuffle.partitions`\n",
    "    - `spark.executor.memory`\n",
    "    - `spark.catalog.listTables()`\n",
    "    - `spark.catalog.listColumns(\"someTable\")`\n",
    "    - `spark.udf.register(\"someName\", someUdf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/09 20:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hands-on-pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112b39ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"hands-on-pyspark\")\n",
    "        .config(\"spark.executor.memory\",\"2g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #ALL, OFF, ERROR, DEBUG, INFO, WARN\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2g'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'hands-on-pyspark'),\n",
       " ('spark.executor.memory', '2g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.startTime', '1728517000110'),\n",
       " ('spark.app.submitTime', '1728517000005'),\n",
       " ('spark.driver.port', '50343'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1728517000757'),\n",
       " ('spark.driver.host', '192.168.100.26'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/marcelomedel/Library/CloudStorage/GoogleDrive-marcelo.medel.v@gmail.com/My%20Drive/github_repos/personal/clases/pyspark-de-usach-v2/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un DataFrame es una estructura de datos bidimensional similar a cualquier tabla en una base de datos estructurada. Algunas de las características que tiene un DataFrame en PySpark son:\n",
    "- **Distribuido**: Los datos están distribuidos en un clúster de nodos, lo que permite el procesamiento paralelo.\n",
    "\n",
    "- **Inmutable**: Cada transformación produce un nuevo DataFrame.\n",
    "\n",
    "- **SQL**: Permite operaciones tipo SQL y ofrece una interfaz similar a Pandas pero a gran escala.\n",
    "\n",
    "- **Conexiones diversas**: Puede leer datos de múltiples fuentes (ej: JSON, CSV, Parquet, JDBC).\n",
    "\n",
    "- **Optimización Automática**: Utiliza *Catalyst Optimizer* para optimizar automáticamente las consultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    {\n",
    "    \"nombre\":\"Marcelo\",\n",
    "    \"nacimiento\": date(1987,10,7),\n",
    "    \"mail\":\"marcelo@gmail.com\",\n",
    "    \"x\":7990,\n",
    "    \"y\":15.26,\n",
    "    \"active\":True\n",
    "    },\n",
    "    {\n",
    "    \"nombre\":\"Andrea\",\n",
    "    \"nacimiento\": date(1994,6,1),\n",
    "    \"mail\":\"andrea@gmail.com\",\n",
    "    \"x\":15860,\n",
    "    \"y\":-10.15,\n",
    "    \"active\":False\n",
    "    },{\n",
    "    \"nombre\":\"Juan\",\n",
    "    \"nacimiento\": date(2000,1,1),\n",
    "    \"mail\":\"juan@gmail.com\",\n",
    "    \"x\":8520,\n",
    "    \"y\":26.15,\n",
    "    \"active\":True\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[active: boolean, mail: string, nacimiento: date, nombre: string, x: bigint, y: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------+-------+-----+------+\n",
      "|active|             mail|nacimiento| nombre|    x|     y|\n",
      "+------+-----------------+----------+-------+-----+------+\n",
      "|  true|marcelo@gmail.com|1987-10-07|Marcelo| 7990| 15.26|\n",
      "| false| andrea@gmail.com|1994-06-01| Andrea|15860|-10.15|\n",
      "|  true|   juan@gmail.com|2000-01-01|   Juan| 8520| 26.15|\n",
      "+------+-----------------+----------+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(active=True, mail='marcelo@gmail.com', nacimiento=datetime.date(1987, 10, 7), nombre='Marcelo', x=7990, y=15.26),\n",
       " Row(active=False, mail='andrea@gmail.com', nacimiento=datetime.date(1994, 6, 1), nombre='Andrea', x=15860, y=-10.15),\n",
       " Row(active=True, mail='juan@gmail.com', nacimiento=datetime.date(2000, 1, 1), nombre='Juan', x=8520, y=26.15)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+------+------+------+\n",
      "| nombre|nacimiento|             mail|     x|     y|active|\n",
      "+-------+----------+-----------------+------+------+------+\n",
      "|Marcelo|1987-10-01|marcelo@gmail.com|  2356| 15.69|  true|\n",
      "| Andrea|2000-01-01| andrea@gmail.com|  5982| 15.69|  true|\n",
      "|   Juan|1994-11-01|   juan@gmail.com|146584|-58.69| false|\n",
      "+-------+----------+-----------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        Row(nombre=\"Marcelo\", nacimiento=date(1987,10,1), mail=\"marcelo@gmail.com\", x=2356, y=15.69, active=True),\n",
    "        Row(nombre=\"Andrea\", nacimiento=date(2000,1,1), mail=\"andrea@gmail.com\", x=5982, y=15.69, active=True),\n",
    "        Row(nombre=\"Juan\", nacimiento=date(1994,11,1), mail=\"juan@gmail.com\", x=146584, y=-58.69, active=False),\n",
    "    ]\n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- active: boolean (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+----+---+---+------+\n",
      "|nombre|nombre_completo|nacimiento|mail|  x|  y|active|\n",
      "+------+---------------+----------+----+---+---+------+\n",
      "+------+---------------+----------+----+---+---+------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- nombre_completo: struct (nullable = true)\n",
      " |    |-- nombre: string (nullable = false)\n",
      " |    |-- apellido: string (nullable = false)\n",
      " |-- nacimiento: date (nullable = false)\n",
      " |-- mail: string (nullable = false)\n",
      " |-- x: integer (nullable = false)\n",
      " |-- y: float (nullable = false)\n",
      " |-- active: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructField, # un campo estructurado\n",
    "    StringType, # texto\n",
    "    DateType, # datetime.date\n",
    "    IntegerType, # enteros\n",
    "    FloatType, # 2.23\n",
    "    BooleanType, # true/false\n",
    "    StructType, # colección (array) de campos estructurados\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"nombre_completo\", StructType([\n",
    "        StructField(\"nombre\", StringType(),False),\n",
    "        StructField(\"apellido\", StringType(),False),\n",
    "    ]) ),\n",
    "    StructField(\"nacimiento\", DateType(), False),\n",
    "    StructField(\"mail\", StringType(), False),\n",
    "    StructField(\"x\", IntegerType(), False),\n",
    "    StructField(\"y\", FloatType(), False),\n",
    "    StructField(\"active\", BooleanType(), False),\n",
    "]\n",
    "\n",
    "schema = StructType(cols)\n",
    "\n",
    "df3 = spark.createDataFrame([],schema)\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- nombre_completo: struct (nullable = true)\n",
      " |    |-- nombre: string (nullable = false)\n",
      " |    |-- apellido: string (nullable = false)\n",
      " |-- nacimiento: date (nullable = false)\n",
      " |-- mail: string (nullable = false)\n",
      " |-- x: integer (nullable = false)\n",
      " |-- y: float (nullable = false)\n",
      " |-- active: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    {\n",
    "    \"nombre\":\"Marcelo\",\n",
    "    \"nombre_completo\":[\"Marcelo\",\"Medel\"],\n",
    "    \"nacimiento\": date(1987,10,7),\n",
    "    \"mail\":\"marcelo@gmail.com\",\n",
    "    \"x\":7990,\n",
    "    \"y\":15.26,\n",
    "    \"active\":True\n",
    "    },\n",
    "    {\n",
    "    \"nombre\":\"Andrea\",\n",
    "    \"nombre_completo\":[\"Andrea\",\"Vergara\"],\n",
    "    \"nacimiento\": date(1994,6,1),\n",
    "    \"mail\":\"andrea@gmail.com\",\n",
    "    \"x\":15860,\n",
    "    \"y\":-10.15,\n",
    "    \"active\":False\n",
    "    },{\n",
    "    \"nombre\":\"Juan\",\n",
    "    \"nombre_completo\":[\"Juan\",\"Bautista\"],\n",
    "    \"nacimiento\": date(2000,1,1),\n",
    "    \"mail\":\"juan@gmail.com\",\n",
    "    \"x\":8520,\n",
    "    \"y\":26.15,\n",
    "    \"active\":True\n",
    "    },\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas sobre un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select, Columns y Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Select*** permite seleccionar un set de columnas y también puede ser usado para renombrar o aplicar *expresiones* a las columnas.\n",
    "\n",
    "***Columns*** en Spark hace referencia a cualquier columna conocida en algun RDBMS, planilla de excel, pandas dataframes, etc. Estas columnas pueden ser seleccionadas, manipuladas o removidas de un DataFrame, por lo que estas operaciones son representadas como ***expressions***\n",
    "\n",
    "-  `col` y `column` son utilizadas para referencias a columnas en un DataFrame\n",
    "- `expr` es utilizado para crear expresiones mas complejas, recibe un string con SQL para ejecutar la operación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literals, withColumn, withColumnRenamed y alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter, where, drop y cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame to Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-pandas](./files/spark_pandas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
