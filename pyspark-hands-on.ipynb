{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Clase - Computación Distribuida\n",
    "\n",
    "    ## Pyspark Hands-on \n",
    "    #### Marcelo Medel Vergara - Diplomado Data Engineer USACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizas Anaconda para administrar ambientes de desarrollo la mejor vía para asegurar que funcionen correctamente las librerías es instalando directo desde Conda. Adicionalmente se instalan todas las librerías que necesita Spark.\n",
    "- `conda create -n pyspark-DE`\n",
    "- `conda activate pyspark-DE`\n",
    "- `conda install -c conda-forge pyspark python=3.10`\n",
    "\n",
    "De igual forma se puede instalar a través de PIP, pero sin asegurar funcionamiento correcto ni la instalación de dependencias. Adicionalmente será necesario tener instalado Pandas.\n",
    "- `conda create -n pyspark-DE python=3.10`\n",
    "- `conda activate pyspark-DE`\n",
    "- `pip install pyspark`\n",
    "- `pip install pandas`.\n",
    "\n",
    "Para Google Colab:\n",
    "- `!pip install pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession es una clase en PySpark que existe desde la versión 2.0 (2016) que simplifica la forma de trabajar con Spark, tanto en las configuraciones como en la manipulación de datos estructurados. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcionalidades principales de SparkSession:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Configura Spark**: Para profundizar en las configuraciones posibles de Spark, visite https://spark.apache.org/docs/latest/configuration.html.\n",
    "    - `SparkSession.builder.appName(\"some name\").**config(\"some config key,value\")**.getOrCreate()`\n",
    "    - `spark.conf.get(\"some config key\")`\n",
    "\n",
    "2. **Crear DataFrames**: permite leer y escribir (Input/Output) diversas fuentes de datos y crear DataFrames para la manipulación de datos.\n",
    "    - `spark.createDataFrame(data [, schema])`\n",
    "    - `spark.read.json(\"path to some json\")`\n",
    "\n",
    "3. **Ejecutar SQL**: facilita la ejecución de consultas en SQL sobre los DataFrames.\n",
    "    - `spark.sql(\"query to some view\")`\n",
    "    \n",
    "4. **Gestiona contexto de Spark**: facilita la configuración y acceso a diferentes componentes y funcionalidades de Spark\n",
    "    - `spark.sql.shuffle.partitions`\n",
    "    - `spark.executor.memory`\n",
    "    - `spark.catalog.listTables()`\n",
    "    - `spark.catalog.listColumns(\"someTable\")`\n",
    "    - `spark.udf.register(\"someName\", someUdf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/09 20:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hands-on-pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112b39ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"hands-on-pyspark\")\n",
    "        .config(\"spark.executor.memory\",\"2g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #ALL, OFF, ERROR, DEBUG, INFO, WARN\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2g'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'hands-on-pyspark'),\n",
       " ('spark.executor.memory', '2g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.startTime', '1728517000110'),\n",
       " ('spark.app.submitTime', '1728517000005'),\n",
       " ('spark.driver.port', '50343'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1728517000757'),\n",
       " ('spark.driver.host', '192.168.100.26'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/marcelomedel/Library/CloudStorage/GoogleDrive-marcelo.medel.v@gmail.com/My%20Drive/github_repos/personal/clases/pyspark-de-usach-v2/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un DataFrame es una estructura de datos bidimensional similar a cualquier tabla en una base de datos estructurada. Algunas de las características que tiene un DataFrame en PySpark son:\n",
    "- **Distribuido**: Los datos están distribuidos en un clúster de nodos, lo que permite el procesamiento paralelo.\n",
    "\n",
    "- **Inmutable**: Cada transformación produce un nuevo DataFrame.\n",
    "\n",
    "- **SQL**: Permite operaciones tipo SQL y ofrece una interfaz similar a Pandas pero a gran escala.\n",
    "\n",
    "- **Conexiones diversas**: Puede leer datos de múltiples fuentes (ej: JSON, CSV, Parquet, JDBC).\n",
    "\n",
    "- **Optimización Automática**: Utiliza *Catalyst Optimizer* para optimizar automáticamente las consultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    {\n",
    "    \"nombre\":\"Marcelo\",\n",
    "    \"nacimiento\": date(1987,10,7),\n",
    "    \"mail\":\"marcelo@gmail.com\",\n",
    "    \"x\":7990,\n",
    "    \"y\":15.26,\n",
    "    \"active\":True\n",
    "    },\n",
    "    {\n",
    "    \"nombre\":\"Andrea\",\n",
    "    \"nacimiento\": date(1994,6,1),\n",
    "    \"mail\":\"andrea@gmail.com\",\n",
    "    \"x\":15860,\n",
    "    \"y\":-10.15,\n",
    "    \"active\":False\n",
    "    },{\n",
    "    \"nombre\":\"Juan\",\n",
    "    \"nacimiento\": date(2000,1,1),\n",
    "    \"mail\":\"juan@gmail.com\",\n",
    "    \"x\":8520,\n",
    "    \"y\":26.15,\n",
    "    \"active\":True\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[active: boolean, mail: string, nacimiento: date, nombre: string, x: bigint, y: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------+-------+-----+------+\n",
      "|active|             mail|nacimiento| nombre|    x|     y|\n",
      "+------+-----------------+----------+-------+-----+------+\n",
      "|  true|marcelo@gmail.com|1987-10-07|Marcelo| 7990| 15.26|\n",
      "| false| andrea@gmail.com|1994-06-01| Andrea|15860|-10.15|\n",
      "|  true|   juan@gmail.com|2000-01-01|   Juan| 8520| 26.15|\n",
      "+------+-----------------+----------+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(active=True, mail='marcelo@gmail.com', nacimiento=datetime.date(1987, 10, 7), nombre='Marcelo', x=7990, y=15.26),\n",
       " Row(active=False, mail='andrea@gmail.com', nacimiento=datetime.date(1994, 6, 1), nombre='Andrea', x=15860, y=-10.15),\n",
       " Row(active=True, mail='juan@gmail.com', nacimiento=datetime.date(2000, 1, 1), nombre='Juan', x=8520, y=26.15)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+------+------+------+\n",
      "| nombre|nacimiento|             mail|     x|     y|active|\n",
      "+-------+----------+-----------------+------+------+------+\n",
      "|Marcelo|1987-10-01|marcelo@gmail.com|  2356| 15.69|  true|\n",
      "| Andrea|2000-01-01| andrea@gmail.com|  5982| 15.69|  true|\n",
      "|   Juan|1994-11-01|   juan@gmail.com|146584|-58.69| false|\n",
      "+-------+----------+-----------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        Row(nombre=\"Marcelo\", nacimiento=date(1987,10,1), mail=\"marcelo@gmail.com\", x=2356, y=15.69, active=True),\n",
    "        Row(nombre=\"Andrea\", nacimiento=date(2000,1,1), mail=\"andrea@gmail.com\", x=5982, y=15.69, active=True),\n",
    "        Row(nombre=\"Juan\", nacimiento=date(1994,11,1), mail=\"juan@gmail.com\", x=146584, y=-58.69, active=False),\n",
    "    ]\n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- active: boolean (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+----+---+---+------+\n",
      "|nombre|nombre_completo|nacimiento|mail|  x|  y|active|\n",
      "+------+---------------+----------+----+---+---+------+\n",
      "+------+---------------+----------+----+---+---+------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- nombre_completo: struct (nullable = true)\n",
      " |    |-- nombre: string (nullable = false)\n",
      " |    |-- apellido: string (nullable = false)\n",
      " |-- nacimiento: date (nullable = false)\n",
      " |-- mail: string (nullable = false)\n",
      " |-- x: integer (nullable = false)\n",
      " |-- y: float (nullable = false)\n",
      " |-- active: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructField, # un campo estructurado\n",
    "    StringType, # texto\n",
    "    DateType, # datetime.date\n",
    "    IntegerType, # enteros\n",
    "    FloatType, # 2.23\n",
    "    BooleanType, # true/false\n",
    "    StructType, # colección (array) de campos estructurados\n",
    "    DecimalType, # DecimalType(10,2)\n",
    "    ArrayType, # lista, tupla, ...\n",
    "    LongType, \n",
    "    DoubleType,\n",
    "    MapType\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"nombre_completo\", StructType([\n",
    "        StructField(\"nombre\", StringType(),False),\n",
    "        StructField(\"apellido\", StringType(),False),\n",
    "    ]) ),\n",
    "    StructField(\"nacimiento\", DateType(), False),\n",
    "    StructField(\"mail\", StringType(), False),\n",
    "    StructField(\"x\", IntegerType(), False),\n",
    "    StructField(\"y\", FloatType(), False),\n",
    "    StructField(\"active\", BooleanType(), False),\n",
    "]\n",
    "\n",
    "schema = StructType(cols)\n",
    "\n",
    "df3 = spark.createDataFrame([],schema)\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- nombre_completo: struct (nullable = true)\n",
      " |    |-- nombre: string (nullable = false)\n",
      " |    |-- apellido: string (nullable = false)\n",
      " |-- nacimiento: date (nullable = false)\n",
      " |-- mail: string (nullable = false)\n",
      " |-- x: integer (nullable = false)\n",
      " |-- y: float (nullable = false)\n",
      " |-- active: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    {\n",
    "    \"nombre\":\"Marcelo\",\n",
    "    \"nombre_completo\":[\"Marcelo\",\"Medel\"],\n",
    "    \"nacimiento\": date(1987,10,7),\n",
    "    \"mail\":\"marcelo@gmail.com\",\n",
    "    \"x\":7990,\n",
    "    \"y\":15.26,\n",
    "    \"active\":True\n",
    "    },\n",
    "    {\n",
    "    \"nombre\":\"Andrea\",\n",
    "    \"nombre_completo\":[\"Andrea\",\"Vergara\"],\n",
    "    \"nacimiento\": date(1994,6,1),\n",
    "    \"mail\":\"andrea@gmail.com\",\n",
    "    \"x\":15860,\n",
    "    \"y\":-10.15,\n",
    "    \"active\":False\n",
    "    },{\n",
    "    \"nombre\":\"Juan\",\n",
    "    \"nombre_completo\":[\"Juan\",\"Bautista\"],\n",
    "    \"nacimiento\": date(2000,1,1),\n",
    "    \"mail\":\"juan@gmail.com\",\n",
    "    \"x\":8520,\n",
    "    \"y\":26.15,\n",
    "    \"active\":True\n",
    "    },\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas sobre un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select, Columns y Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Select*** permite seleccionar un set de columnas y también puede ser usado para renombrar o aplicar *expresiones* a las columnas.\n",
    "\n",
    "***Columns*** en Spark hace referencia a cualquier columna conocida en algun RDBMS, planilla de excel, pandas dataframes, etc. Estas columnas pueden ser seleccionadas, manipuladas o removidas de un DataFrame, por lo que estas operaciones son representadas como ***expressions***\n",
    "\n",
    "-  `col` y `column` son utilizadas para referencias a columnas en un DataFrame\n",
    "- `expr` es utilizado para crear expresiones mas complejas, recibe un string con SQL para ejecutar la operación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-----------------+-----------------+\n",
      "|    x|nacimiento|     y|  nombre_completo|  nombre_completo|\n",
      "+-----+----------+------+-----------------+-----------------+\n",
      "| 7990|1987-10-07| 15.26| {Marcelo, Medel}| {Marcelo, Medel}|\n",
      "|15860|1994-06-01|-10.15|{Andrea, Vergara}|{Andrea, Vergara}|\n",
      "| 8520|2000-01-01| 26.15| {Juan, Bautista}| {Juan, Bautista}|\n",
      "+-----+----------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column, expr\n",
    "\n",
    "df.select('x'\n",
    "          ,df.nacimiento\n",
    "          ,col(\"y\")\n",
    "          ,column(\"nombre_completo\")\n",
    "          ,expr(\"nombre_completo\")\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+-------+\n",
      "| nombre|apellido|apellido|apellido| nombre|\n",
      "+-------+--------+--------+--------+-------+\n",
      "|Marcelo|   Medel|   Medel|   Medel|Marcelo|\n",
      "| Andrea| Vergara| Vergara| Vergara| Andrea|\n",
      "|   Juan|Bautista|Bautista|Bautista|   Juan|\n",
      "+-------+--------+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('nombre_completo.*'\n",
    "          ,'nombre_completo.apellido'\n",
    "          ,col('nombre_completo.apellido')\n",
    "          ,expr('nombre_completo.nombre')\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+--------------------+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|valor_iva|                 asd|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+--------------------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|    11008|[marcelo, gmail.com]|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|    20373| [andrea, gmail.com]|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|    11639|   [juan, gmail.com]|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    expr(\"*\"),\n",
    "    expr(\"round(x*1.19)+1500 as valor_iva\"),\n",
    "    expr(\"split(mail,'@') as asd\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select(\n",
    "    expr(\"*\"),\n",
    "    expr(\"round(x*1.19)+1500 as valor_iva\"),\n",
    "    expr(\"split(mail,'@') as asd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|valor_iva|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|    11008|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|    20373|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|    11639|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\",\"round(x*1.19)+1500 as valor_iva\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------+\n",
      "|avg(valor_iva)|max(valor_iva)|min(valor_iva)|\n",
      "+--------------+--------------+--------------+\n",
      "|    14340.0000|         20373|         11008|\n",
      "+--------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\",\"round(x*1.19)+1500 as valor_iva\").selectExpr(\"avg(valor_iva)\", \"max(valor_iva)\", \"min(valor_iva)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literals, withColumn, withColumnRenamed y alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+----+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|flag|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+----+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|   1|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|   1|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|   1|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+----+\n",
      "\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+----+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|flag|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+----+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|   1|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|   1|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|   1|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"flag\")).show()\n",
    "\n",
    "df.selectExpr(\"*\", \"1 as flag\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df3 \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mround(x*1.19) as valor_iva\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df3\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3267\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   3266\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 3267\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2751\u001b[0m, in \u001b[0;36mDataFrame._jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jseq\u001b[39m(\n\u001b[1;32m   2746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2747\u001b[0m     cols: Sequence,\n\u001b[1;32m   2748\u001b[0m     converter: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m, JavaObject]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2749\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/pyspark/sql/column.py:90\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     88\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_gateway.py:1314\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1314\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[0;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[1;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_collections.py:511\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 511\u001b[0m     \u001b[43mjava_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_gateway.py:1314\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1314\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[0;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[1;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/py4j/java_collections.py:510\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    508\u001b[0m ArrayList \u001b[38;5;241m=\u001b[39m JavaClass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.util.ArrayList\u001b[39m\u001b[38;5;124m\"\u001b[39m, gateway_client)\n\u001b[1;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[0;32m--> 510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    511\u001b[0m     java_list\u001b[38;5;241m.\u001b[39madd(element)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark-v2/lib/python3.10/site-packages/pyspark/sql/column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    720\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "df3 = df.selectExpr(\"*\", expr(\"round(x*1.19) as valor_iva\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "| nombre|  nombre_completo|nacimiento|             mail|    x|     y|active|valor_iva|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|     9508|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|    18873|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|    10139|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.withColumn(\"flag\",lit(2)).show()\n",
    "df2 = df.withColumn(\"valor_iva\",expr(\"round(x*1.19)\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "|Marcelo|\n",
      "| Andrea|\n",
      "|   Juan|\n",
      "+-------+\n",
      "\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "| nombre|         nombre_2|nacimiento|             mail|    x|     y|active|valor_iva|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|     9508|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|    18873|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|    10139|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumnRenamed(\"nombre_completo\",\"nombre_2\").select(\"nombre_2.nombre\").show()\n",
    "df2 = df2.withColumnRenamed(\"nombre_completo\",\"nombre_2\")\n",
    "df2.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "| nombre|         nombre_2|nacimiento|             mail| pago|   uso|active|valor_iva|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "|Marcelo| {Marcelo, Medel}|1987-10-07|marcelo@gmail.com| 7990| 15.26|  true|     9508|\n",
      "| Andrea|{Andrea, Vergara}|1994-06-01| andrea@gmail.com|15860|-10.15| false|    18873|\n",
      "|   Juan| {Juan, Bautista}|2000-01-01|   juan@gmail.com| 8520| 26.15|  true|    10139|\n",
      "+-------+-----------------+----------+-----------------+-----+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumnsRenamed(\n",
    "            {\"nombre_completo\":\"nombre2\",\n",
    "                        \"x\":\"pago\",\n",
    "                        \"y\":'uso'}\n",
    "                       )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter, where, drop y cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame to Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-pandas](./files/spark_pandas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
