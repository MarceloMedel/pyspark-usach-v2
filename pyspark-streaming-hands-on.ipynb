{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase - Computación Distribuida\n",
    "\n",
    "## Pyspark Streaming Hands-on \n",
    "#### Marcelo Medel Vergara - Diplomado Data Engineer USACH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Sources\n",
    "\n",
    "Structured Streaming nos permite recibir datos desde distintas fuentes de datos:\n",
    "\n",
    "- Apache Kafka 0.10 - https://kafka.apache.org/ \n",
    "- Archivos en un sistema de archivos distribuidos \n",
    "    - HDFS - https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html \n",
    "    - Amazon - S3 https://aws.amazon.com/es/s3/ \n",
    "- Socket local para propósitos de testing - https://docs.python.org/3/library/socket.html \n",
    "\n",
    "### Sinks \n",
    "\n",
    "Un sink determina dónde y cómo se almacenan los resultados del procesamiento de streaming\n",
    "\n",
    "- **File Sink**: útil para escribir los resultados del procesamiento en archivos (CSV, Parquet, JSON).\n",
    "\n",
    "- **Console Sink**: Los resultados del procesamiento se imprimen en la consola de salida de PySpark.\n",
    "\n",
    "- **Kafka Sink**: Se puede enviar los resultados del procesamiento a Kafka.\n",
    "\n",
    "- **JDBC Sink**: Puede escribir los datos en una tabla de una base de datos compatible con JDBC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/16 20:40:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112cb6a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"streaming\").getOrCreate()\n",
    "\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- conn_country: string (nullable = true)\n",
      " |-- episode_name: string (nullable = true)\n",
      " |-- episode_show_name: string (nullable = true)\n",
      " |-- incognito_mode: boolean (nullable = true)\n",
      " |-- ip_addr_decrypted: string (nullable = true)\n",
      " |-- master_metadata_album_album_name: string (nullable = true)\n",
      " |-- master_metadata_album_artist_name: string (nullable = true)\n",
      " |-- master_metadata_track_name: string (nullable = true)\n",
      " |-- ms_played: long (nullable = true)\n",
      " |-- offline: boolean (nullable = true)\n",
      " |-- offline_timestamp: long (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- reason_end: string (nullable = true)\n",
      " |-- reason_start: string (nullable = true)\n",
      " |-- shuffle: boolean (nullable = true)\n",
      " |-- skipped: string (nullable = true)\n",
      " |-- spotify_episode_uri: string (nullable = true)\n",
      " |-- spotify_track_uri: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- user_agent_decrypted: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n",
      "+------------+------------+-----------------+--------------+-----------------+--------------------------------+---------------------------------+--------------------------+---------+-------+-----------------+--------------------+----------+------------+-------+-------+-------------------+--------------------+--------------------+--------------------+-----------+\n",
      "|conn_country|episode_name|episode_show_name|incognito_mode|ip_addr_decrypted|master_metadata_album_album_name|master_metadata_album_artist_name|master_metadata_track_name|ms_played|offline|offline_timestamp|            platform|reason_end|reason_start|shuffle|skipped|spotify_episode_uri|   spotify_track_uri|                  ts|user_agent_decrypted|   username|\n",
      "+------------+------------+-----------------+--------------+-----------------+--------------------------------+---------------------------------+--------------------------+---------+-------+-----------------+--------------------+----------+------------+-------+-------+-------------------+--------------------+--------------------+--------------------+-----------+\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|       Show Me How to Live|   277840|  false|    1485613394490|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:4j4...|2017-01-28T14:35:07Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|                  Gasoline|   279333|  false|    1485614093978|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:6ze...|2017-01-28T14:40:01Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|              What You Are|   249400|  false|    1485614400206|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:5D4...|2017-01-28T14:44:10Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|              Like a Stone|   293960|  false|    1485614649671|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:2Mr...|2017-01-28T14:49:04Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|                Set It Off|   263200|  false|    1485614943660|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:0OL...|2017-01-28T14:53:51Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|         Shadow on the Sun|   343400|  false|    1485615230427|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:1NI...|2017-01-28T15:00:02Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|          I Am the Highway|   334906|  false|    1485615600639|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:72Z...|2017-01-28T15:44:41Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                      Audioslave|                       Audioslave|                  Exploder|    58409|  false|    1485618280420|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:5Ce...|2017-01-28T15:45:40Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|            How You Sell Soul...|                     Public Enemy|      Harder Than You T...|   249521|  false|    1485618340134|iOS 10.2 (iPhone8,1)|      NULL|    clickrow|  false|   NULL|               NULL|spotify:track:5Yp...|2017-01-28T15:49:50Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|            Fear Of A Black P...|                     Public Enemy|           Fight The Power|   282640|  false|    1485618589237|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:26z...|2017-01-28T15:54:35Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                     He Got Game|                     Public Enemy|               He Got Game|   286200|  false|    1485618872287|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:7aR...|2017-01-28T15:59:36Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|            Apocalypse 91… Th...|                     Public Enemy|            Can't Truss It|   321560|  false|    1485619174529|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:54C...|2017-01-28T16:08:11Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                 Made In Chicago|                     Public Enemy|           Bring the Noise|    97559|  false|    1485619690535|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:6Le...|2017-01-28T16:09:50Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|            The Day Is My Ene...|                      The Prodigy|               Shut 'Em Up|   260000|  false|    1485619789483|iOS 10.2 (iPhone8,1)|      NULL|      fwdbtn|  false|   NULL|               NULL|spotify:track:6uz...|2017-01-28T16:14:09Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|            It Takes A Nation...|                     Public Enemy|           Bring The Noise|   226040|  false|    1485620048572|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:2bK...|2017-01-28T16:17:55Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|            It Takes A Nation...|                     Public Enemy|      Don't Believe The...|    53084|  false|    1485620274933|iOS 10.2 (iPhone8,1)|      NULL|   trackdone|  false|   NULL|               NULL|spotify:track:0PL...|2017-01-28T18:28:21Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                   Greatest Hits|                           N.W.A.|      Straight Outta Co...|   230968|  false|    1485628100619|iOS 10.2 (iPhone8,1)|      NULL|    clickrow|  false|   NULL|               NULL|spotify:track:3eG...|2017-01-28T18:32:12Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                           Hello|                            Adele|                     Hello|     1648|  false|    1485628332165|iOS 10.2 (iPhone8,1)|      NULL|    clickrow|   true|   NULL|               NULL|spotify:track:0EN...|2017-01-28T18:32:15Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                         La Bala|                       Ana Tijoux|              Sacar la Voz|     1021|  false|    1485628334241|iOS 10.2 (iPhone8,1)|      NULL|      fwdbtn|   true|   NULL|               NULL|spotify:track:1tR...|2017-01-28T18:32:16Z|                NULL|12135236562|\n",
      "|          CL|        NULL|             NULL|         false|             NULL|                              18|                             Moby|             In This World|      766|  false|    1485628335311|iOS 10.2 (iPhone8,1)|      NULL|  trackerror|   true|   NULL|               NULL|spotify:track:5cG...|2017-01-28T18:32:17Z|                NULL|12135236562|\n",
      "+------------+------------+-----------------+--------------+-----------------+--------------------------------+---------------------------------+--------------------------+---------+-------+-----------------+--------------------+----------+------------+-------+-------+-------------------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.json(\"data/streaming/streaming.json\")\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, TimestampType\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# schema = StructType([\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# data_stream = spark.readStream.schema(schema).json(\"data/streaming/\")\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/streaming/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/sql/streaming/readwriter.py:410\u001b[0m, in \u001b[0;36mDataStreamReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, locale, dropFieldIfAllNull, encoding, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    387\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    388\u001b[0m     primitivesAsString\u001b[38;5;241m=\u001b[39mprimitivesAsString,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m     allowNonNumericNumbers\u001b[38;5;241m=\u001b[39mallowNonNumericNumbers,\n\u001b[1;32m    408\u001b[0m )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    413\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    415\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"conn_country\", StringType(), True),\n",
    "    StructField(\"episode_name\", StringType(), True),\n",
    "    StructField(\"episode_show_name\", StringType(), True),\n",
    "    StructField(\"incognito_mode\", BooleanType(), True),\n",
    "    StructField(\"ip_addr_decrypted\", StringType(), True),\n",
    "    StructField(\"master_metadata_album_album_name\", StringType(), True),\n",
    "    StructField(\"master_metadata_album_artist_name\", StringType(), True),\n",
    "    StructField(\"master_metadata_track_name\", StringType(), True),\n",
    "    StructField(\"ms_played\", LongType(), True),\n",
    "    StructField(\"offline\", BooleanType(), True),\n",
    "    StructField(\"offline_timestamp\", LongType(), True),\n",
    "    StructField(\"platform\", StringType(), True),\n",
    "    StructField(\"reason_end\", StringType(), True),\n",
    "    StructField(\"reason_start\", StringType(), True),\n",
    "    StructField(\"shuffle\", BooleanType(), True),\n",
    "    StructField(\"skipped\", BooleanType(), True),\n",
    "    StructField(\"spotify_episode_uri\", StringType(), True),\n",
    "    StructField(\"spotify_track_uri\", StringType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True),\n",
    "    StructField(\"user_agent_decrypted\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True)\n",
    "])\n",
    "\n",
    "data_stream = spark.readStream.schema(schema).json(\"data/streaming/\")\n",
    "data_stream = spark.readStream.json(\"data/streaming/\")\n",
    "data_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output modes\n",
    "\n",
    "- **Complete mode**:\n",
    "    - Envía todo el resultado calculado al destino.\n",
    "    - Útil para datos de estado que cambian con el tiempo.\n",
    "    - Útil cuando el destino no admite actualizaciones a nivel de fila.\n",
    "- **Update mode**:\n",
    "    - Envía solo las filas que difieren de la última escritura al destino.\n",
    "    - Destino debe admitir actualizaciones a nivel de fila.\n",
    "    - Si la consulta no contiene agregaciones, es equivalente al modo de *append*\n",
    "- **Append mode**:\n",
    "    - Nuevas filas se envían al destino especificado\n",
    "    - Garantiza que cada fila se envíe una vez y solo una vez\n",
    "    - Destino debe ser tolerante a fallos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers\n",
    "\n",
    "Los triggers en Spark Structured Streaming controlan cuándo se envía la data al destino. Por defecto, el streaming comienza a procesar datos tan pronto como el trigger anterior termina. Los triggers son útiles para evitar sobrecargar el destino con demasiadas actualizaciones o para controlar el tamaño de los archivos de salida. Existen dos tipos de triggers:\n",
    "\n",
    "- **Processing Time Trigger**: Se especifica una duración (ej: “10 segundos”) y Spark esperará múltiplos de esa duración para enviar los datos. Si el procesamiento no termina antes del siguiente *trigger*, Spark esperará al siguiente punto en lugar de disparar inmediatamente.\n",
    "- **Once Trigger**: Permite ejecutar un trabajo de streaming solo una vez. Es útil tanto en desarrollo (para probar aplicaciones con un solo conjunto de datos) como en producción (para ejecutar trabajos manualmente a una baja frecuencia, ahorrando recursos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming from socket\n",
    "\n",
    "Usar este código para simular envío de mensajes a través de un socket --> https://github.com/MarceloMedel/SPARK-DE-USACH/blob/main/streaming_file.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115696b30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"streaming-socket\").getOrCreate()\n",
    "\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
