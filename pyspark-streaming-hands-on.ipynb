{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase - Computación Distribuida\n",
    "\n",
    "## Pyspark Streaming Hands-on \n",
    "#### Marcelo Medel Vergara - Diplomado Data Engineer USACH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Sources\n",
    "\n",
    "Structured Streaming nos permite recibir datos desde distintas fuentes de datos:\n",
    "\n",
    "- Apache Kafka 0.10 - https://kafka.apache.org/ \n",
    "- Archivos en un sistema de archivos distribuidos \n",
    "    - HDFS - https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html \n",
    "    - Amazon - S3 https://aws.amazon.com/es/s3/ \n",
    "- Socket local para propósitos de testing - https://docs.python.org/3/library/socket.html \n",
    "\n",
    "### Sinks \n",
    "\n",
    "Un sink determina dónde y cómo se almacenan los resultados del procesamiento de streaming\n",
    "\n",
    "- **File Sink**: útil para escribir los resultados del procesamiento en archivos (CSV, Parquet, JSON).\n",
    "\n",
    "- **Console Sink**: Los resultados del procesamiento se imprimen en la consola de salida de PySpark.\n",
    "\n",
    "- **Kafka Sink**: Se puede enviar los resultados del procesamiento a Kafka.\n",
    "\n",
    "- **JDBC Sink**: Puede escribir los datos en una tabla de una base de datos compatible con JDBC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/26 21:56:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115696b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"streaming\").getOrCreate()\n",
    "\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output modes\n",
    "\n",
    "- **Complete mode**:\n",
    "    - Envía todo el resultado calculado al destino.\n",
    "    - Útil para datos de estado que cambian con el tiempo.\n",
    "    - Útil cuando el destino no admite actualizaciones a nivel de fila.\n",
    "- **Update mode**:\n",
    "    - Envía solo las filas que difieren de la última escritura al destino.\n",
    "    - Destino debe admitir actualizaciones a nivel de fila.\n",
    "    - Si la consulta no contiene agregaciones, es equivalente al modo de *append*\n",
    "- **Append mode**:\n",
    "    - Nuevas filas se envían al destino especificado\n",
    "    - Garantiza que cada fila se envíe una vez y solo una vez\n",
    "    - Destino debe ser tolerante a fallos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers\n",
    "\n",
    "Los triggers en Spark Structured Streaming controlan cuándo se envía la data al destino. Por defecto, el streaming comienza a procesar datos tan pronto como el trigger anterior termina. Los triggers son útiles para evitar sobrecargar el destino con demasiadas actualizaciones o para controlar el tamaño de los archivos de salida. Existen dos tipos de triggers:\n",
    "\n",
    "- **Processing Time Trigger**: Se especifica una duración (ej: “10 segundos”) y Spark esperará múltiplos de esa duración para enviar los datos. Si el procesamiento no termina antes del siguiente *trigger*, Spark esperará al siguiente punto en lugar de disparar inmediatamente.\n",
    "- **Once Trigger**: Permite ejecutar un trabajo de streaming solo una vez. Es útil tanto en desarrollo (para probar aplicaciones con un solo conjunto de datos) como en producción (para ejecutar trabajos manualmente a una baja frecuencia, ahorrando recursos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming from socket\n",
    "\n",
    "Usar este código para simular envío de mensajes a través de un socket --> https://github.com/MarceloMedel/SPARK-DE-USACH/blob/main/streaming_file.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115696b30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"streaming-socket\").getOrCreate()\n",
    "\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
